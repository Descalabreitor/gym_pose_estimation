{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from torchvision.io import read_video\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T11:28:07.603331500Z",
     "start_time": "2023-12-15T11:28:07.602325Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [],
   "source": [
    "MODELS_DIR = \"/models\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T11:28:07.628950800Z",
     "start_time": "2023-12-15T11:28:07.603331500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, data_path, packet_len):\n",
    "        self.data_folder = data_path\n",
    "        self.n_frames_per_batch = packet_len\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Get the list of subfolders (classes)\n",
    "        self.classes = sorted(os.listdir(data_path))\n",
    "\n",
    "        # Map classes to numeric indices\n",
    "        self.class_to_index = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "        # List to store video paths and their labels\n",
    "        self.data = []\n",
    "\n",
    "        # Iterate through subfolders and create the data list\n",
    "        for cls in self.classes:\n",
    "            cls_path = os.path.join(data_path, cls)\n",
    "            if os.path.isdir(cls_path):\n",
    "                videos = os.listdir(cls_path)\n",
    "                for video in videos:\n",
    "                    video_path = os.path.join(cls_path, video)\n",
    "                    self.data.append((video_path, self.class_to_index[cls]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        video_path, label = self.data[index]\n",
    "        coordinates_tensor = self.preprocess(video_path, self.n_frames_per_batch)\n",
    "\n",
    "        return coordinates_tensor, label\n",
    "\n",
    "    def extract_pose(self, frame_package):\n",
    "        mp_pose = mp.solutions.pose\n",
    "    \n",
    "        # Inference poses in the frame package using mediapipe\n",
    "        with mp_pose.Pose(min_detection_confidence=0.4, min_tracking_confidence=0.4) as pose:\n",
    "            pose_data = []\n",
    "            for frame in frame_package:\n",
    "                frame = frame.permute(1, 2, 0)\n",
    "                result = pose.process(np.array(frame).astype(np.uint8))\n",
    "    \n",
    "                # Check if pose is detected\n",
    "                if result.pose_landmarks:\n",
    "                    # Extract relative coordinates of landmarks\n",
    "                    landmarks = [[landmark.x, landmark.y, landmark.z] for landmark in result.pose_landmarks.landmark]\n",
    "                else:\n",
    "                    # If no pose is detected, set all landmarks to 0\n",
    "                    landmarks = [[0, 0, 0] for _ in range(33)]\n",
    "    \n",
    "                pose_data.append(landmarks)\n",
    "    \n",
    "        return pose_data\n",
    "\n",
    "    def preprocess(self, video_path, n):\n",
    "        # Read the video using torchvision\n",
    "        video, _, _ = read_video(video_path, pts_unit='sec')\n",
    "    \n",
    "        # Total number of frames in the video\n",
    "        total_frames = video.size(0)\n",
    "    \n",
    "        # Ensure that the video has at least 'n' frames\n",
    "        if total_frames < n:\n",
    "            raise ValueError(f\"The video has fewer than {n} frames.\")\n",
    "    \n",
    "        # Randomly select the starting index for the segment\n",
    "        start_idx = random.randint(0, total_frames - n)\n",
    "    \n",
    "        if start_idx + n > total_frames - 1:\n",
    "            start_idx = start_idx - (start_idx + n - total_frames - 1)\n",
    "    \n",
    "        # Extract the frame package from the randomly selected segment\n",
    "        frame_package = video[start_idx:start_idx + n].permute(0, 3, 1, 2)\n",
    "    \n",
    "        pose_data = self.extract_pose(frame_package)\n",
    "    \n",
    "        return torch.tensor(pose_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T11:28:07.629950200Z",
     "start_time": "2023-12-15T11:28:07.612385100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "def load_data(data_folder, len_package, batch_size, shuffle=True):\n",
    "    # Assuming VideoDataset is your custom dataset class\n",
    "    dataset = VideoDataset(data_folder, len_package)\n",
    "\n",
    "    # Calculate sizes for train, validation, and test sets\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(0.8 * total_size)\n",
    "    val_size = (total_size - train_size) // 2\n",
    "    test_size = total_size - train_size - val_size\n",
    "\n",
    "    # Use random_split to split the dataset\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        dataset, [train_size, val_size, test_size]\n",
    "    )\n",
    "    train_data_loader =DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    test_data_loader =DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    return train_data_loader, val_data_loader, test_data_loader\n",
    "\n",
    "# Example usage\n",
    "data_folder = \"data\"  # Path to the folder containing subfolders for each class\n",
    "frames_per_package = 10\n",
    "batch_size = 32\n",
    "train_dataloader, val_dataloader, test_dataloader = load_data(data_folder, frames_per_package, batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T11:28:07.629950200Z",
     "start_time": "2023-12-15T11:28:07.617221Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Net architectures\n",
    "\n",
    "Vamos a hacer 3 arquitecturas de distintos tamaños. Se trata solo de la capa fully conected."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,frames_per_package, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(frames_per_package * 33 * 3, 3000)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(3000,300)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(300, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        out = self.softmax(self.fc3(x))\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T11:28:07.630951400Z",
     "start_time": "2023-12-15T11:28:07.623513Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def accuracy(all_labels, all_pred):\n",
    "    correct_predictions = np.sum(np.array(all_labels) == np.array(all_pred))\n",
    "    total_predictions = len(all_labels)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "def f1_score(all_labels, all_pred, num_classes=3):\n",
    "    confusion_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "\n",
    "    for true_label, predicted_label in zip(all_labels, all_pred):\n",
    "        confusion_matrix[true_label][predicted_label] += 1\n",
    "\n",
    "    precision = np.zeros(num_classes, dtype=float)\n",
    "    recall = np.zeros(num_classes, dtype=float)\n",
    "    f1 = np.zeros(num_classes, dtype=float)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        true_positive = confusion_matrix[i, i]\n",
    "        false_positive = np.sum(confusion_matrix[:, i]) - true_positive\n",
    "        false_negative = np.sum(confusion_matrix[i, :]) - true_positive\n",
    "\n",
    "        if true_positive + false_positive == 0:\n",
    "            precision[i] = 0\n",
    "        else:\n",
    "            precision[i] = true_positive / (true_positive + false_positive)\n",
    "\n",
    "        if true_positive + false_negative == 0:\n",
    "            recall[i] = 0\n",
    "        else:\n",
    "            recall[i] = true_positive / (true_positive + false_negative)\n",
    "\n",
    "        if precision[i] + recall[i] == 0:\n",
    "            f1[i] = 0\n",
    "        else:\n",
    "            f1[i] = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "\n",
    "    # Promedio de F1 scores de todas las clases (macro-F1)\n",
    "    macro_f1 = np.mean(f1)\n",
    "\n",
    "    return macro_f1\n",
    "\n",
    "# función de entrenamiento\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "\n",
    "    model.train()\n",
    "    all_pred = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch, (inputs, labels) in enumerate(dataloader):\n",
    "\n",
    "        inputs, labels = inputs.to(device), torch.tensor(labels).to(device)\n",
    "        print(inputs.shape, labels.shape)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(inputs)\n",
    "        loss = loss_fn(pred, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _ ,pred_classes = torch.max(pred, 1)\n",
    "\n",
    "        all_pred.extend(pred_classes.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy(all_labels, all_pred)\n",
    "    f1 = f1_score(all_labels, all_pred)\n",
    "    mean_loss = total_loss/len(dataloader)\n",
    "\n",
    "    print(f\"Training accuraccy: {acc:.4f} | Training F1-Score: {f1:.4f} | Training mean_loss: {mean_loss:.4f}\\n-------------------------------\")\n",
    "\n",
    "    return acc, f1, mean_loss\n",
    "\n",
    "#Validate and Test Func\n",
    "def validate(dataloader, model, loss_fn):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    all_pred = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            pred = model(inputs)\n",
    "            loss = loss_fn(pred, labels)\n",
    "            _ ,pred_classes = torch.max(pred, 1)\n",
    "\n",
    "            all_pred.extend(pred_classes.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    acc = accuracy(all_labels, all_pred)\n",
    "    f1 = f1_score(all_labels, all_pred)\n",
    "    return acc, f1, total_loss/len(dataloader)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_metrics(train_acc, val_acc, train_f1, val_f1, train_loss, val_loss):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    # Graficar train_acc y val_acc en la primera subtrama\n",
    "    ax1.plot(train_acc, label='train_acc', color='blue')\n",
    "    ax1.plot(val_acc, label='val_acc', color='green')\n",
    "    ax1.set_title('Train vs Validation Accuracy')\n",
    "    ax1.set_xlabel('Época')\n",
    "    ax1.set_ylabel('Precisión')\n",
    "    ax1.legend()\n",
    "\n",
    "    # Graficar train_f1 y val_f1 en la segunda subtrama\n",
    "    ax2.plot(train_f1, label='train_f1', color='blue')\n",
    "    ax2.plot(val_f1, label='val_f1', color='green')\n",
    "    ax2.set_title('Train vs Validation F1 Score')\n",
    "    ax2.set_xlabel('Época')\n",
    "    ax2.set_ylabel('Puntuación F1')\n",
    "    ax2.legend()\n",
    "\n",
    "    # Graficar train_loss y val_loss en la tercera subtrama\n",
    "    ax3.plot(train_loss, label='train_loss', color='blue')\n",
    "    ax3.plot(val_loss, label='val_loss', color='green')\n",
    "    ax3.set_title('Train vs Validation Loss')\n",
    "    ax3.set_xlabel('Época')\n",
    "    ax3.set_ylabel('Pérdida')\n",
    "    ax3.legend()\n",
    "\n",
    "    # Ajustar el espaciado entre las subtramas\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Mostrar el gráfico\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T11:28:07.640946200Z",
     "start_time": "2023-12-15T11:28:07.634956800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [10, 33, 3] at entry 0 and [9, 33, 3] at entry 9",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[188], line 30\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mEPOCHS\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m-------------------------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     29\u001B[0m \u001B[38;5;66;03m# Training phase\u001B[39;00m\n\u001B[1;32m---> 30\u001B[0m acc, f1, loss \u001B[38;5;241m=\u001B[39m train(train_dataloader, model, loss_fn, optimizer)\n\u001B[0;32m     32\u001B[0m train_acc\u001B[38;5;241m.\u001B[39mappend(acc)\n\u001B[0;32m     33\u001B[0m train_f1\u001B[38;5;241m.\u001B[39mappend(f1)\n",
      "Cell \u001B[1;32mIn[187], line 52\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(dataloader, model, loss_fn, optimizer)\u001B[0m\n\u001B[0;32m     49\u001B[0m all_labels \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     50\u001B[0m total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m---> 52\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch, (inputs, labels) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataloader):\n\u001B[0;32m     54\u001B[0m     inputs, labels \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(device), torch\u001B[38;5;241m.\u001B[39mtensor(labels)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     55\u001B[0m     \u001B[38;5;28mprint\u001B[39m(inputs\u001B[38;5;241m.\u001B[39mshape, labels\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cuda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[0;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cuda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    672\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    673\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 674\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_fetcher\u001B[38;5;241m.\u001B[39mfetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    675\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    676\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cuda\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcollate_fn(data)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cuda\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001B[0m, in \u001B[0;36mdefault_collate\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m    204\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[0;32m    205\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001B[39;00m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    263\u001B[0m \u001B[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[0;32m    264\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 265\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m collate(batch, collate_fn_map\u001B[38;5;241m=\u001B[39mdefault_collate_fn_map)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cuda\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    139\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    141\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m--> 142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [collate(samples, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    144\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cuda\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    139\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    141\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m--> 142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [collate(samples, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    144\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cuda\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    117\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m collate_fn_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    118\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m elem_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[1;32m--> 119\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map)\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m collate_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[0;32m    122\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collate_type):\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cuda\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001B[0m, in \u001B[0;36mcollate_tensor_fn\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    160\u001B[0m     storage \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39m_typed_storage()\u001B[38;5;241m.\u001B[39m_new_shared(numel, device\u001B[38;5;241m=\u001B[39melem\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    161\u001B[0m     out \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39mnew(storage)\u001B[38;5;241m.\u001B[39mresize_(\u001B[38;5;28mlen\u001B[39m(batch), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlist\u001B[39m(elem\u001B[38;5;241m.\u001B[39msize()))\n\u001B[1;32m--> 162\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mstack(batch, \u001B[38;5;241m0\u001B[39m, out\u001B[38;5;241m=\u001B[39mout)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: stack expects each tensor to be equal size, but got [10, 33, 3] at entry 0 and [9, 33, 3] at entry 9"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Model(frames_per_package, num_classes=22)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "# Number of epochs\n",
    "EPOCHS = 5\n",
    "\n",
    "# Lists to store training and validation results\n",
    "train_acc = []\n",
    "train_f1 = []\n",
    "train_loss = []\n",
    "\n",
    "\n",
    "val_acc = []\n",
    "val_f1 = []\n",
    "val_loss = []\n",
    "\n",
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch {t + 1}/{EPOCHS}\\n-------------------------------\")\n",
    "\n",
    "    # Training phase\n",
    "    acc, f1, loss = train(train_dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "    train_acc.append(acc)\n",
    "    train_f1.append(f1)\n",
    "    train_loss.append(loss)\n",
    "\n",
    "    # Validation phase\n",
    "    acc, f1, loss = validate(val_dataloader, model, loss_fn)\n",
    "    val_acc.append(acc)\n",
    "    val_f1.append(f1)\n",
    "    val_loss.append(loss)\n",
    "    scheduler.step(loss)\n",
    "\n",
    "    print(f\"Validation accuraccy: {acc:.3f} | Validation F1-Score: {f1:.3f} | Validation mean_loss: {loss:.4f}\\n-------------------------------\")\n",
    "\n",
    "\n",
    "print(\"Training and Validation Done!\")\n",
    "\n",
    "test_acc, test_f1, test_loss = validate(test_dataloader, model, loss_fn)\n",
    "\n",
    "print(f\"Test accuraccy: {test_acc:.3f} | Test F1-Score: {test_f1:.3f} | Test loss: {test_loss:.3f}\\n-------------------------------\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), MODELS_DIR + \"pre_model_cnn.pth\")\n",
    "print(\"Model saved as model_cnn.pth\")\n",
    "draw_metrics(train_acc, val_acc, train_f1, val_f1, train_loss, val_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T11:28:40.168935500Z",
     "start_time": "2023-12-15T11:28:07.637947400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T11:28:40.169934700Z",
     "start_time": "2023-12-15T11:28:40.169934700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T11:28:40.169934700Z",
     "start_time": "2023-12-15T11:28:40.169934700Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AAAIV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
