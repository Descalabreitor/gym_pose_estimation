{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from torchvision.io import read_video\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T12:55:37.607077500Z",
     "start_time": "2023-12-16T12:55:37.596058600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "MODELS_DIR = \"/models/\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T15:46:47.842978700Z",
     "start_time": "2023-12-16T15:46:47.840851500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, data_path, packet_len):\n",
    "        self.data_folder = data_path\n",
    "        self.n_frames_per_batch = packet_len\n",
    "\n",
    "        # Get the list of subfolders (classes)\n",
    "        self.classes = sorted(os.listdir(data_path))\n",
    "\n",
    "        # Map classes to numeric indices\n",
    "        self.class_to_index = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "        # List to store video paths and their labels\n",
    "        self.data = []\n",
    "\n",
    "        # Iterate through subfolders and create the data list\n",
    "        for cls in self.classes:\n",
    "            cls_path = os.path.join(data_path, cls)\n",
    "            if os.path.isdir(cls_path):\n",
    "                videos = os.listdir(cls_path)\n",
    "                for video in videos:\n",
    "                    video_path = os.path.join(cls_path, video)\n",
    "                    self.data.append((video_path, self.class_to_index[cls]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        video_path, label = self.data[index]\n",
    "        coordinates_tensor = self.preprocess(video_path, self.n_frames_per_batch)\n",
    "\n",
    "        return coordinates_tensor, label\n",
    "\n",
    "    def extract_pose(self, frame_package):\n",
    "        mp_pose = mp.solutions.pose\n",
    "    \n",
    "        # Inference poses in the frame package using mediapipe\n",
    "        with mp_pose.Pose(min_detection_confidence=0.4, min_tracking_confidence=0.4) as pose:\n",
    "            pose_data = []\n",
    "            for frame in frame_package:\n",
    "                frame = frame.permute(1, 2, 0)\n",
    "                result = pose.process(np.array(frame).astype(np.uint8))\n",
    "    \n",
    "                # Check if pose is detected\n",
    "                if result.pose_landmarks:\n",
    "                    # Extract relative coordinates of landmarks\n",
    "                    landmarks = [[landmark.x, landmark.y, landmark.z] for landmark in result.pose_landmarks.landmark]\n",
    "                else:\n",
    "                    # If no pose is detected, set all landmarks to 0\n",
    "                    landmarks = [[0, 0, 0] for _ in range(33)]\n",
    "    \n",
    "                pose_data.append(landmarks)\n",
    "    \n",
    "        return pose_data\n",
    "\n",
    "    def preprocess(self, video_path, n):\n",
    "        # Read the video using torchvision\n",
    "        video, _, _ = read_video(video_path, pts_unit='sec')\n",
    "    \n",
    "        # Total number of frames in the video\n",
    "        total_frames = video.size(0)\n",
    "    \n",
    "        # Ensure that the video has at least 'n' frames\n",
    "        if total_frames < n:\n",
    "            raise ValueError(f\"The video has fewer than {n} frames.\")\n",
    "    \n",
    "        # Randomly select the starting index for the segment\n",
    "        start_idx = random.randint(0, total_frames - n)\n",
    "    \n",
    "        if start_idx + n > total_frames:\n",
    "            start_idx = start_idx - (start_idx + n - total_frames)\n",
    "    \n",
    "        # Extract the frame package from the randomly selected segment\n",
    "        frame_package = video[start_idx:start_idx + n].permute(0, 3, 1, 2)\n",
    "    \n",
    "        pose_data = self.extract_pose(frame_package)\n",
    "    \n",
    "        return torch.tensor(pose_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T12:55:37.637854500Z",
     "start_time": "2023-12-16T12:55:37.611732300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "def load_data(data_folder, len_package, batch_size, shuffle=True):\n",
    "    # Assuming VideoDataset is your custom dataset class\n",
    "    dataset = VideoDataset(data_folder, len_package)\n",
    "\n",
    "    # Calculate sizes for train, validation, and test sets\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(0.8 * total_size)\n",
    "    val_size = (total_size - train_size) // 2\n",
    "    test_size = total_size - train_size - val_size\n",
    "\n",
    "    # Use random_split to split the dataset\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        dataset, [train_size, val_size, test_size]\n",
    "    )\n",
    "    train_data_loader =DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    test_data_loader =DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    return train_data_loader, val_data_loader, test_data_loader\n",
    "\n",
    "# Example usage\n",
    "data_folder = \"data\"  # Path to the folder containing subfolders for each class\n",
    "frames_per_package = 10\n",
    "batch_size = 64\n",
    "train_dataloader, val_dataloader, test_dataloader = load_data(data_folder, frames_per_package, batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T12:55:37.637854500Z",
     "start_time": "2023-12-16T12:55:37.618592200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Net architectures\n",
    "\n",
    "Vamos a hacer 3 arquitecturas de distintos tamaños. Se trata solo de la capa fully conected."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,frames_per_package, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(frames_per_package * 33 * 3, 3000)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(3000,300)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(300, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        out = self.softmax(self.fc3(x))\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T12:55:37.638860200Z",
     "start_time": "2023-12-16T12:55:37.629113800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def accuracy(all_labels, all_pred):\n",
    "    correct_predictions = np.sum(np.array(all_labels) == np.array(all_pred))\n",
    "    total_predictions = len(all_labels)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "# función de entrenamiento\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "\n",
    "    model.train()\n",
    "    all_pred = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch, (inputs, labels) in enumerate(dataloader):\n",
    "\n",
    "        inputs, labels = inputs.to(device), torch.tensor(labels).to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(inputs)\n",
    "        loss = loss_fn(pred, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _ ,pred_classes = torch.max(pred, 1)\n",
    "\n",
    "        all_pred.extend(pred_classes.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy(all_labels, all_pred)\n",
    "    mean_loss = total_loss/len(dataloader)\n",
    "\n",
    "    print(f\"Training accuraccy: {acc:.4f} | Training mean_loss: {mean_loss:.4f}\\n-------------------------------\")\n",
    "\n",
    "    return acc, mean_loss\n",
    "\n",
    "#Validate and Test Func\n",
    "def validate(dataloader, model, loss_fn):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    all_pred = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            pred = model(inputs)\n",
    "            loss = loss_fn(pred, labels)\n",
    "            _ ,pred_classes = torch.max(pred, 1)\n",
    "\n",
    "            all_pred.extend(pred_classes.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    acc = accuracy(all_labels, all_pred)\n",
    "    return acc, total_loss/len(dataloader)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_metrics(train_acc, val_acc, train_loss, val_loss):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    # Graficar train_acc y val_acc en la primera subtrama\n",
    "    ax1.plot(train_acc, label='train_acc', color='blue')\n",
    "    ax1.plot(val_acc, label='val_acc', color='green')\n",
    "    ax1.set_title('Train vs Validation Accuracy')\n",
    "    ax1.set_xlabel('Época')\n",
    "    ax1.set_ylabel('Precisión')\n",
    "    ax1.legend()\n",
    "\n",
    "    # Graficar train_loss y val_loss en la tercera subtrama\n",
    "    ax3.plot(train_loss, label='train_loss', color='blue')\n",
    "    ax3.plot(val_loss, label='val_loss', color='green')\n",
    "    ax3.set_title('Train vs Validation Loss')\n",
    "    ax3.set_xlabel('Época')\n",
    "    ax3.set_ylabel('Pérdida')\n",
    "    ax3.legend()\n",
    "\n",
    "    # Ajustar el espaciado entre las subtramas\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Mostrar el gráfico\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T12:55:37.640861900Z",
     "start_time": "2023-12-16T12:55:37.631615100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adria\\AppData\\Local\\Temp\\ipykernel_21120\\2231422014.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs, labels = inputs.to(device), torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuraccy: 0.0729 | Training mean_loss: 3.0907\n",
      "-------------------------------\n",
      "Validation accuraccy: 0.092 | Validation mean_loss: 3.0913\n",
      "-------------------------------\n",
      "Epoch 2/10\n",
      "-------------------------------\n",
      "Training accuraccy: 0.0845 | Training mean_loss: 3.0908\n",
      "-------------------------------\n",
      "Validation accuraccy: 0.062 | Validation mean_loss: 3.0906\n",
      "-------------------------------\n",
      "Epoch 3/10\n",
      "-------------------------------\n",
      "Training accuraccy: 0.0921 | Training mean_loss: 3.0906\n",
      "-------------------------------\n",
      "Validation accuraccy: 0.077 | Validation mean_loss: 3.0913\n",
      "-------------------------------\n",
      "Epoch 4/10\n",
      "-------------------------------\n",
      "Training accuraccy: 0.0921 | Training mean_loss: 3.0905\n",
      "-------------------------------\n",
      "Validation accuraccy: 0.077 | Validation mean_loss: 3.0889\n",
      "-------------------------------\n",
      "Epoch 5/10\n",
      "-------------------------------\n",
      "Training accuraccy: 0.0902 | Training mean_loss: 3.0904\n",
      "-------------------------------\n",
      "Validation accuraccy: 0.077 | Validation mean_loss: 3.0908\n",
      "-------------------------------\n",
      "Epoch 6/10\n",
      "-------------------------------\n",
      "Training accuraccy: 0.0940 | Training mean_loss: 3.0901\n",
      "-------------------------------\n",
      "Validation accuraccy: 0.123 | Validation mean_loss: 3.0901\n",
      "-------------------------------\n",
      "Epoch 7/10\n",
      "-------------------------------\n",
      "Training accuraccy: 0.1017 | Training mean_loss: 3.0902\n",
      "-------------------------------\n",
      "Validation accuraccy: 0.077 | Validation mean_loss: 3.0877\n",
      "-------------------------------\n",
      "Epoch 8/10\n",
      "-------------------------------\n",
      "Training accuraccy: 0.1056 | Training mean_loss: 3.0901\n",
      "-------------------------------\n",
      "Validation accuraccy: 0.092 | Validation mean_loss: 3.0898\n",
      "-------------------------------\n",
      "Epoch 9/10\n",
      "-------------------------------\n",
      "Training accuraccy: 0.1228 | Training mean_loss: 3.0899\n",
      "-------------------------------\n",
      "Validation accuraccy: 0.092 | Validation mean_loss: 3.0877\n",
      "-------------------------------\n",
      "Epoch 10/10\n",
      "-------------------------------\n",
      "Training accuraccy: 0.1401 | Training mean_loss: 3.0900\n",
      "-------------------------------\n",
      "Validation accuraccy: 0.092 | Validation mean_loss: 3.0879\n",
      "-------------------------------\n",
      "Training and Validation Done!\n",
      "Test accuraccy: 0.091| Test loss: 3.089\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "File /modelspre_model_cnn.pth cannot be opened.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 51\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest accuraccy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_acc\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m| Test loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m-------------------------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     50\u001B[0m \u001B[38;5;66;03m# Save the trained model\u001B[39;00m\n\u001B[1;32m---> 51\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(model\u001B[38;5;241m.\u001B[39mstate_dict(), MODELS_DIR \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpre_model_cnn.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel saved as model_cnn.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     53\u001B[0m draw_metrics(train_acc, val_acc, train_loss, val_loss)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cuda\\Lib\\site-packages\\torch\\serialization.py:618\u001B[0m, in \u001B[0;36msave\u001B[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001B[0m\n\u001B[0;32m    615\u001B[0m _check_save_filelike(f)\n\u001B[0;32m    617\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _use_new_zipfile_serialization:\n\u001B[1;32m--> 618\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m _open_zipfile_writer(f) \u001B[38;5;28;01mas\u001B[39;00m opened_zipfile:\n\u001B[0;32m    619\u001B[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001B[0;32m    620\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cuda\\Lib\\site-packages\\torch\\serialization.py:492\u001B[0m, in \u001B[0;36m_open_zipfile_writer\u001B[1;34m(name_or_buffer)\u001B[0m\n\u001B[0;32m    490\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    491\u001B[0m     container \u001B[38;5;241m=\u001B[39m _open_zipfile_writer_buffer\n\u001B[1;32m--> 492\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m container(name_or_buffer)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cuda\\Lib\\site-packages\\torch\\serialization.py:463\u001B[0m, in \u001B[0;36m_open_zipfile_writer_file.__init__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m    461\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39mPyTorchFileWriter(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfile_stream))\n\u001B[0;32m    462\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 463\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39mPyTorchFileWriter(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname))\n",
      "\u001B[1;31mRuntimeError\u001B[0m: File /modelspre_model_cnn.pth cannot be opened."
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Model(frames_per_package, num_classes=22)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "# Number of epochs\n",
    "EPOCHS = 10\n",
    "\n",
    "# Lists to store training and validation results\n",
    "train_acc = []\n",
    "\n",
    "train_loss = []\n",
    "\n",
    "\n",
    "val_acc = []\n",
    "\n",
    "val_loss = []\n",
    "\n",
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch {t + 1}/{EPOCHS}\\n-------------------------------\")\n",
    "\n",
    "    # Training phase\n",
    "    acc, loss = train(train_dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "    train_acc.append(acc)\n",
    "    train_loss.append(loss)\n",
    "\n",
    "    # Validation phase\n",
    "    acc, loss = validate(val_dataloader, model, loss_fn)\n",
    "    val_acc.append(acc)\n",
    "    val_loss.append(loss)\n",
    "    scheduler.step(loss)\n",
    "\n",
    "    print(f\"Validation accuraccy: {acc:.3f} | Validation mean_loss: {loss:.4f}\\n-------------------------------\")\n",
    "\n",
    "\n",
    "print(\"Training and Validation Done!\")\n",
    "\n",
    "test_acc, test_loss = validate(test_dataloader, model, loss_fn)\n",
    "\n",
    "print(f\"Test accuraccy: {test_acc:.3f}| Test loss: {test_loss:.3f}\\n-------------------------------\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), MODELS_DIR + \"pre_model_cnn.pth\")\n",
    "print(\"Model saved as model_cnn.pth\")\n",
    "draw_metrics(train_acc, val_acc, train_loss, val_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T15:39:09.917960500Z",
     "start_time": "2023-12-16T12:55:37.640861900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models\\\\pre_model_cnn.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T15:47:38.637897300Z",
     "start_time": "2023-12-16T15:47:38.602134500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T15:39:09.921960500Z",
     "start_time": "2023-12-16T15:39:09.920959400Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AAAIV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
