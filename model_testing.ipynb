{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T23:43:31.979667700Z",
     "start_time": "2023-12-14T23:43:29.675069600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the coordinates tensor: torch.Size([7, 10, 33, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from torchvision.io import read_video\n",
    "from torchvision.transforms import ToPILImage, ToTensor, Compose\n",
    "\n",
    "\n",
    "def preprocess(video_path, n):\n",
    "    # Initialize the mediapipe model for pose estimation\n",
    "    mp_pose = mp.solutions.pose\n",
    "\n",
    "    # Extract the video label from the path (name without extension)\n",
    "    label = video_path.split('/')[-2]\n",
    "\n",
    "    # Read the video using torchvision\n",
    "    video, _, _ = read_video(video_path, pts_unit='sec')\n",
    "\n",
    "    # Total number of frames in the video\n",
    "    total_frames = video.size(0)\n",
    "\n",
    "    # List to store matrices of relative coordinates\n",
    "    data = []\n",
    "\n",
    "    # Iterate over frames with a step of 'n'\n",
    "    for i in range(0, total_frames, n):\n",
    "        # Make sure not to go out of range\n",
    "        end_idx = min(i + n, total_frames)\n",
    "\n",
    "        # Extract the frame package\n",
    "        frame_package = video[i:end_idx].permute(0, 3, 1, 2)\n",
    "        \n",
    "        # Inference poses in the frame package using mediapipe\n",
    "        with mp_pose.Pose() as pose:\n",
    "            pose_data = []\n",
    "            for frame in frame_package:\n",
    "                frame =frame.permute(1, 2, 0)\n",
    "                result = pose.process(np.array(frame).astype(np.uint8))\n",
    "                # Extract relative coordinates of landmarks \n",
    "                landmarks = [[landmark.x, landmark.y, landmark.z] for landmark in result.pose_landmarks.landmark]\n",
    "                pose_data.append(landmarks)\n",
    "                \n",
    "        data.append(torch.tensor(pose_data))\n",
    "\n",
    "    # If the last package is smaller, pad with zeros\n",
    "    if len(data) > 1:\n",
    "        last_package_size = len(data[-1])\n",
    "        if last_package_size < n:\n",
    "            padding_size = n - last_package_size\n",
    "            zero_padding = torch.zeros(padding_size, len(data[-1][0]))\n",
    "            data[-1] = torch.cat([data[-1], zero_padding])\n",
    "\n",
    "    return torch.stack(data), label\n",
    "\n",
    "# Example usage\n",
    "video_path = \"data/barbell biceps curl/barbell biceps curl_61.mp4\"\n",
    "n_frames_per_package = 10\n",
    "\n",
    "tensor_coordinates, label = preprocess(video_path, n_frames_per_package)\n",
    "\n",
    "# Print the shape of the resulting tensor\n",
    "print(f\"Shape of the coordinates tensor: {tensor_coordinates.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, data_folder, n_frames_per_batch):\n",
    "        self.data_folder = data_folder\n",
    "        self.n_frames_per_batch = n_frames_per_batch\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Get the list of subfolders (classes)\n",
    "        self.classes = sorted(os.listdir(data_folder))\n",
    "\n",
    "        # Map classes to numeric indices\n",
    "        self.class_to_index = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "        # List to store video paths and their labels\n",
    "        self.data = []\n",
    "\n",
    "        # Iterate through subfolders and create the data list\n",
    "        for cls in self.classes:\n",
    "            cls_path = os.path.join(data_folder, cls)\n",
    "            if os.path.isdir(cls_path):\n",
    "                videos = os.listdir(cls_path)\n",
    "                for video in videos:\n",
    "                    video_path = os.path.join(cls_path, video)\n",
    "                    self.data.append((video_path, self.class_to_index[cls]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        video_path, label = self.data[index]\n",
    "        coordinates_tensor, label = preprocess(video_path, self.n_frames_per_batch)\n",
    "        \n",
    "        return coordinates_tensor, label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_data(data_folder, n_frames_per_batch, batch_size, shuffle=True):\n",
    "    dataset = VideoDataset(data_folder, n_frames_per_batch)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return data_loader\n",
    "\n",
    "# Example usage\n",
    "data_folder = \"data\"  # Path to the folder containing subfolders for each class\n",
    "n_frames_per_batch = 10\n",
    "batch_size = 32\n",
    "\n",
    "data_loader = load_data(data_folder, n_frames_per_batch, batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AAAIV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
